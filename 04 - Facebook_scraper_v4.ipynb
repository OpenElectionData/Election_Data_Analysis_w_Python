{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Facebook Public Page Scraper\n",
    "<p class=\"lead\">\n",
    "Michelle Brown Notes v 1.4<br />\n",
    "\n",
    "\n",
    "This notebook scrapes the inforamtion from a public Facebook page using Facebook's Graph API (2.6).\n",
    "\n",
    "This will scrape all the posts and comments of a Facebook Public Page and the related metadata, including post message, post links, and counts of each reaction on the post. All this data is exported as a CSV, so it can be imported into an analysis program like Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import some Python dependencies\n",
    "import urllib2\n",
    "import json\n",
    "import datetime\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to access page data, Facebook requires an access token so we are going to create a dummy applications for the sole purspose of scraping. <br>\n",
    "\n",
    "1. Login to your Facebook account, then go to developers.facebook.com and go to the upper right corner and in the dropbox menu, click \"add a new app.\" \n",
    "2. Give the app a name and click Create App ID\n",
    "3. Click on the newly created app to pull up the Dashboard. \n",
    "4. In the Dashboard, you'll see and App ID and an APP secret. These are what you will post in between the QUOTATIONS (\" \") below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "app_id = \"PASTEHERE\" \n",
    "app_secret = \"PASTEHERE\" # DO NOT SHARE THIS WITH ANYONE!\n",
    "\n",
    "access_token = app_id + \"|\" + app_secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can access public Facebook data without limit. Let's scrape the [NDI Facebook Page](https://www.facebook.com/National.Democratic.Institute).  Below you want to replace what is between the quotes (' ') with the ID for the facebook page or group. Once you find the page you want to scrape, cut and paste the url into this tool: <a href=\"https://lookup-id.com/\">https://lookup-id.com/</a> and it will give you the ID to put below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "group_id = 'National.Democratic.Institute'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below pings the Facebook page to verify that the `access_token` works and the `page_id` is valid. If it works, it should output the id and name of the page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testFacebookPageData(group_id, access_token):\n",
    "    \n",
    "    # construct the URL string\n",
    "    base = \"https://graph.facebook.com/v2.6\"\n",
    "    node = \"/\" + group_id   \n",
    "    parameters = \"/?access_token=%s\" % access_token\n",
    "    url = base + node + parameters\n",
    "    \n",
    "    # retrieve data\n",
    "    req = urllib2.Request(url)\n",
    "    response = urllib2.urlopen(req)\n",
    "    data = json.loads(response.read())\n",
    "    \n",
    "    print json.dumps(data, indent=4, sort_keys=True)\n",
    "    \n",
    "testFacebookPageData(group_id, access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When scraping large amounts of data from public APIs, there's a high probability that you'll hit an [HTTP Error 500 (Internal Error)](http://www.checkupdown.com/status/E500.html) at some point. This is a helper function to catch the error and try again after a few seconds, which usually works. This helper function also consolidates the data retrival code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def request_until_succeed(url):\n",
    "    req = urllib2.Request(url)\n",
    "    success = False\n",
    "    while success is False:\n",
    "        try: \n",
    "            response = urllib2.urlopen(req)\n",
    "            if response.getcode() == 200:\n",
    "                success = True\n",
    "        except Exception, e:\n",
    "            print e\n",
    "            time.sleep(5)\n",
    "            \n",
    "            print \"Error for URL %s: %s\" % (url, datetime.datetime.now())\n",
    "\n",
    "    return response.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>We construct the string to make the request from the Facebook API and then test that we get back some data:</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def testFacebookPageFeedData(group_id, access_token):\n",
    "    \n",
    "    # construct the URL string\n",
    "    base = \"https://graph.facebook.com/v2.6\"\n",
    "    node = \"/\" + group_id + \"/feed\"                     # group id\n",
    "    parameters = \"/?access_token=%s\" % access_token\n",
    "    url = base + node + parameters\n",
    "    \n",
    "    # retrieve data\n",
    "    data = json.loads(request_until_succeed(url))\n",
    "    \n",
    "    print json.dumps(data, indent=4, sort_keys=True)\n",
    "    \n",
    "testFacebookPageFeedData(group_id, access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Run the functions to scrape all of the statuses'information and the reactions</b> \n",
    "When you run the code below, one function will get the feed from teh page that has information about the statuses. Another function will get the count of each reaction type for each status. Another function normalizes some of the data (especially statuses prior to 24 of February 2016 where there were fewer reaction types) for dates and times. Another function writes out the data to a comma separated file while also printing an output of the progress of the scraper until it is done running. There are are also a couple of helper functions. One that helps to normalize the unicoe and the other helper function that retrys the API if there is an error. \n",
    "it will use two helper fuctions. Next we need to normalize the unicode so we can put it into a csv file and then we get the info.<br>\n",
    "Once the scraper is finished, it should say it's down and it will write out the data to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Needed to write tricky unicode correctly to csv\n",
    "  ##left single quotation, right single quotation, left double quotation, right double quotation,\n",
    "    #non braking space \n",
    "def unicode_normalize(text):\n",
    "    return text.translate({ 0x2018:0x27, 0x2019:0x27, 0x201C:0x22, 0x201D:0x22,\n",
    "                            0xa0:0x20 }).encode('utf-8')\n",
    "    \n",
    "\n",
    "# Helper function here for server request\n",
    "def request_until_succeed(url):\n",
    "    req = urllib2.Request(url)\n",
    "    success = False\n",
    "    while success is False:\n",
    "        try: \n",
    "            response = urllib2.urlopen(req)\n",
    "            if response.getcode() == 200:\n",
    "                success = True\n",
    "        except Exception, e:\n",
    "            print e\n",
    "            time.sleep(5)\n",
    "            \n",
    "            print \"Error for URL %s: %s\" % (url, datetime.datetime.now())\n",
    "\n",
    "    return response.read()\n",
    "\n",
    "def getFacebookPageFeedData(group_id, access_token, num_statuses):\n",
    "    # Construct the URL string; see \n",
    "    # http://stackoverflow.com/a/37239851 for Reactions parameters\n",
    "    base = \"https://graph.facebook.com/v2.6\"\n",
    "    node = \"/%s/feed\" % group_id \n",
    "    fields = \"/?fields=message,link,created_time,type,name,id,\" + \\\n",
    "            \"comments.limit(0).summary(true),shares,reactions.\" + \\\n",
    "            \"limit(0).summary(true),from\"\n",
    "    parameters = \"&limit=%s&access_token=%s\" % (num_statuses, access_token)\n",
    "    url = base + node + fields + parameters\n",
    "\n",
    "    # retrieve data\n",
    "    data = json.loads(request_until_succeed(url))\n",
    "\n",
    "    return data\n",
    "\n",
    "def getReactionsForStatus(status_id, access_token):\n",
    "    # See http://stackoverflow.com/a/37239851 for Reactions parameters\n",
    "        # Reactions are only accessable at a single-post endpoint\n",
    "\n",
    "    base = \"https://graph.facebook.com/v2.6\"\n",
    "    node = \"/%s\" % status_id\n",
    "    reactions = \"/?fields=\" \\\n",
    "            \"reactions.type(LIKE).limit(0).summary(total_count).as(like)\" \\\n",
    "            \",reactions.type(LOVE).limit(0).summary(total_count).as(love)\" \\\n",
    "            \",reactions.type(WOW).limit(0).summary(total_count).as(wow)\" \\\n",
    "            \",reactions.type(HAHA).limit(0).summary(total_count).as(haha)\" \\\n",
    "            \",reactions.type(SAD).limit(0).summary(total_count).as(sad)\" \\\n",
    "            \",reactions.type(ANGRY).limit(0).summary(total_count).as(angry)\"\n",
    "    parameters = \"&access_token=%s\" % access_token\n",
    "    url = base + node + reactions + parameters\n",
    "\n",
    "    # retrieve data\n",
    "    data = json.loads(request_until_succeed(url))\n",
    "\n",
    "    return data\n",
    "\n",
    "def processFacebookPageFeedStatus(status, access_token):\n",
    "    # The status is now a Python dictionary, so for top-level items,\n",
    "    # we can simply call the key.\n",
    "\n",
    "    # Additionally, some items may not always exist,\n",
    "    # so must check for existence first\n",
    "\n",
    "    status_id = status['id']\n",
    "    status_message = '' if 'message' not in status.keys() else \\\n",
    "            unicode_normalize(status['message'])\n",
    "    link_name = '' if 'name' not in status.keys() else \\\n",
    "            unicode_normalize(status['name'])\n",
    "    status_type = status['type']\n",
    "    status_link = '' if 'link' not in status.keys() else \\\n",
    "            unicode_normalize(status['link'])\n",
    "    status_author = unicode_normalize(status['from']['name'])\n",
    "\n",
    "    # Time needs special care since a) it's in UTC and\n",
    "    # b) it's not easy to use in statistical programs.\n",
    "\n",
    "    status_published = datetime.datetime.strptime(\\\n",
    "            status['created_time'],'%Y-%m-%dT%H:%M:%S+0000')\n",
    "    status_published = status_published + datetime.timedelta(hours=-5) # Adjusting for Eastern Standard Time (EST)\n",
    "    # best time format for spreadsheet programs:\n",
    "    status_published = status_published.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Nested items require chaining dictionary keys.\n",
    "    num_reactions = 0 if 'reactions' not in status else \\\n",
    "            status['reactions']['summary']['total_count']\n",
    "    num_comments = 0 if 'comments' not in status else \\\n",
    "            status['comments']['summary']['total_count']\n",
    "    num_shares = 0 if 'shares' not in status else \\\n",
    "            status['shares']['count']\n",
    "\n",
    "    # Counts of each reaction separately; good for sentiment\n",
    "    # Only check for reactions if past date when implemented:\n",
    "    # http://newsroom.fb.com/news/2016/02/reactions-now-available-globally/\n",
    "\n",
    "    reactions = getReactionsForStatus(status_id, access_token) \\\n",
    "            if status_published > '2016-02-24 00:00:00' else {}\n",
    "\n",
    "    num_likes = 0 if 'like' not in reactions else \\\n",
    "            reactions['like']['summary']['total_count']\n",
    "\n",
    "    # Special case: Set number of Likes to Number of reactions for pre-reaction \n",
    "    # statuses\n",
    "\n",
    "    num_likes = num_reactions if status_published < '2016-02-24 00:00:00' else \\\n",
    "            num_likes\n",
    "\n",
    "    def get_num_total_reactions(reaction_type, reactions):\n",
    "        if reaction_type not in reactions:\n",
    "            return 0\n",
    "        else:\n",
    "            return reactions[reaction_type]['summary']['total_count']\n",
    "\n",
    "    num_loves = get_num_total_reactions('love', reactions)\n",
    "    num_wows = get_num_total_reactions('wow', reactions)\n",
    "    num_hahas = get_num_total_reactions('haha', reactions)\n",
    "    num_sads = get_num_total_reactions('sad', reactions)\n",
    "    num_angrys = get_num_total_reactions('angry', reactions)\n",
    "\n",
    "    # return a tuple of all processed data\n",
    "    return (status_id, status_message, status_author, link_name, status_type, \n",
    "            status_link, status_published, num_reactions, num_comments, \n",
    "            num_shares,  num_likes, num_loves, num_wows, num_hahas, num_sads, \n",
    "            num_angrys)\n",
    "\n",
    "def scrapeFacebookPageFeedStatus(group_id, access_token):\n",
    "    with open('%s_facebook_statuses.csv' % group_id, 'wb') as file:\n",
    "        w = csv.writer(file)\n",
    "        w.writerow([\"status_id\", \"status_message\", \"status_author\", \n",
    "            \"link_name\", \"status_type\", \"status_link\",\n",
    "            \"status_published\", \"num_reactions\", \"num_comments\", \n",
    "            \"num_shares\", \"num_likes\", \"num_loves\", \"num_wows\", \n",
    "            \"num_hahas\", \"num_sads\", \"num_angrys\"])\n",
    "\n",
    "        has_next_page = True\n",
    "        num_processed = 0   # keep a count on how many we've processed for status\n",
    "        scrape_starttime = datetime.datetime.now()\n",
    "\n",
    "        print \"Scraping %s Facebook Page: %s\\n\" % \\\n",
    "                (group_id, scrape_starttime)\n",
    "\n",
    "        statuses = getFacebookPageFeedData(group_id, access_token, 100)\n",
    "\n",
    "        while has_next_page:\n",
    "            for status in statuses['data']:\n",
    "\n",
    "                # Ensure it is a status with the expected metadata\n",
    "                if 'reactions' in status:            \n",
    "                    w.writerow(processFacebookPageFeedStatus(status, \\\n",
    "                                                            access_token))\n",
    "\n",
    "                # After every 100 statuss, print output progress to make sure code is not\n",
    "                # stalling\n",
    "                num_processed += 1\n",
    "                if num_processed % 100 == 0:\n",
    "                    print \"%s Statuses Processed: %s\" % (num_processed, \n",
    "                            datetime.datetime.now())\n",
    "\n",
    "            # if there is no next page, we're done.\n",
    "            if 'paging' in statuses.keys():\n",
    "                statuses = json.loads(request_until_succeed(\\\n",
    "                        statuses['paging']['next']))\n",
    "            else:\n",
    "                has_next_page = False\n",
    "\n",
    "\n",
    "        print \"\\nDone!\\n%s Statuses Processed in %s\" % \\\n",
    "                (num_processed, datetime.datetime.now() - scrape_starttime)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    scrapeFacebookPageFeedStatus(group_id, access_token)\n",
    "\n",
    "# The CSV can be opened in all major statistical programs. Have fun! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should be a csv files saved in the same directory as this notebook (e.g. National.Democratic.Institute_facebook_statuses.csv)<br>\n",
    "The CSV can be opened in all major statistical programs. Analyze and enjoy!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
